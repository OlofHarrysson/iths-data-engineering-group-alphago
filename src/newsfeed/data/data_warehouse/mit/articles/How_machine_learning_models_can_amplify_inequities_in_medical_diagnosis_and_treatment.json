{
  "unique_id": "326387a7-f4f0-5de1-bee6-2d87b46e0224",
  "title": "How machine learning models can amplify inequities in medical diagnosis and treatment",
  "description": "MIT researchers investigate the causes of health-care disparities among underrepresented groups.<br />",
  "link": "https://news.mit.edu/2023/how-machine-learning-models-can-amplify-inequities-medical-diagnosis-treatment-0817",
  "blog_text": "Prior to receiving a PhD in computer science from MIT in 2017, Marzyeh Ghassemi had already begun to wonder whether the use of AI techniques might enhance the biases that already existed in health care. She was one of the early researchers to take up this issue, and she\u2019s been exploring it ever since. In a new paper, Ghassemi, now an assistant professor in MIT\u2019s Department of Electrical Science and Engineering (EECS), and three collaborators based at the Computer Science and Artificial Intelligence Laboratory, have probed the roots of the disparities that can arise in machine learning, often causing models that perform well overall to falter when it comes to subgroups for which relatively few data have been collected and utilized in the training process. The paper \u2014 written by two MIT PhD students, Yuzhe Yang and Haoran Zhang, EECS computer scientist Dina Katabi (the Thuan and Nicole Pham Professor), and Ghassemi \u2014 was presented last month at the 40th International Conference on Machine Learning in Honolulu, Hawaii.\nIn their analysis, the researchers focused on \"subpopulation shifts\" \u2014 differences in the way machine learning models perform for one subgroup as compared to another. \u201cWe want the models to be fair and work equally well for all groups, but instead we consistently observe the presence of shifts among different groups that can lead to inferior medical diagnosis and treatment,\u201d says Yang, who along with Zhang are the two lead authors on the paper. The main point of their inquiry is to determine the kinds of subpopulation shifts that can occur and to uncover the mechanisms behind them so that, ultimately, more equitable models can be developed.\nThe new paper \u201csignificantly advances our understanding\u201d of the subpopulation shift phenomenon, claims Stanford University computer scientist Sanmi Koyejo. \u201cThis research contributes valuable insights for future advancements in machine learning models' performance on underrepresented subgroups.\u201d\nCamels and cattle\nThe MIT group has identified four principal types of shifts \u2014 spurious correlations, attribute imbalance, class imbalance, and attribute generalization \u2014 which, according to Yang, \u201chave never been put together into a coherent and unified framework. We\u2019ve come up with a single equation that shows you where biases can come from.\u201d\nBiases can, in fact, stem from what the researchers call the class, or from the attribute, or both. To pick a simple example, suppose the task assigned to the machine learning model is to sort images of objects \u2014 animals in this case \u2014 into two classes: cows and camels. Attributes are descriptors that don\u2019t specifically relate to the class itself. It might turn out, for instance, that all the images used in the analysis show cows standing on grass and camels on sand \u2014 grass and sand serving as the attributes here. Given the data available to it, the machine could reach an erroneous conclusion \u2014 namely that cows can only be found on grass, not on sand, with the opposite being true for camels. Such a finding would be incorrect, however, giving rise to a spurious correlation, which, Yang explains, is a \u201cspecial case\u201d among subpopulation shifts \u2014 \u201cone in which you have a bias in both the class and the attribute.\u201d\nIn a medical setting, one could rely on machine learning models to determine whether a person has pneumonia or not based on an examination of X-ray images. There would be two classes in this situation, one consisting of people who have the lung ailment, another for those who are infection-free. A relatively straightforward case would involve just two attributes: the people getting X-rayed are either female or male. If, in this particular dataset, there were 100 males diagnosed with pneumonia for every one female diagnosed with pneumonia, that could lead to an attribute imbalance, and the model would likely do a better job of correctly detecting pneumonia for a man than for a woman. Similarly, having 1,000 times more healthy (pneumonia-free) subjects than sick ones would lead to a class imbalance, with the model biased toward healthy cases. Attribute generalization is the last shift highlighted in the new study. If your sample contained 100 male patients with pneumonia and zero female subjects with the same illness, you still would like the model to be able to generalize and make predictions about female subjects even though there are no samples in the training data for females with pneumonia.\nThe team then took 20 advanced algorithms, designed to carry out classification tasks, and tested them on a dozen datasets to see how they performed across different population groups. They reached some unexpected conclusions: By improving the \u201cclassifier,\u201d which is the last layer of the neural network, they were able to reduce the occurrence of spurious correlations and class imbalance, but the other shifts were unaffected. Improvements to the \u201cencoder,\u201d one of the uppermost layers in the neural network, could reduce the problem of attribute imbalance. \u201cHowever, no matter what we did to the encoder or classifier, we did not see any improvements in terms of attribute generalization,\u201d Yang says, \u201cand we don\u2019t yet know how to address that.\u201d\nPrecisely accurate\nThere is also the question of assessing how well your model actually works in terms of evenhandedness among different population groups. The metric normally used, called worst-group accuracy or WGA, is based on the assumption that if you can improve the accuracy \u2014 of, say, medical diagnosis \u2014 for the group that has the worst model performance, you would have improved the model as a whole. \u201cThe WGA is considered the gold standard in subpopulation evaluation,\u201d the authors contend, but they made a surprising discovery: boosting worst-group accuracy results in a decrease in what they call \u201cworst-case precision.\u201d In medical decision-making of all sorts, one needs both accuracy \u2014 which speaks to the validity of the findings \u2014 and precision, which relates to the reliability of the methodology. \u201cPrecision and accuracy are both very important metrics in classification tasks, and that is especially true in medical diagnostics,\u201d Yang explains. \u201cYou should never trade precision for accuracy. You always need to balance the two.\u201d\nThe MIT scientists are putting their theories into practice. In a study they're conducting with a medical center, they\u2019re looking at public datasets for tens of thousands of patients and hundreds of thousands of chest X-rays, trying to see whether it\u2019s possible for machine learning models to work in an unbiased manner for all populations. That\u2019s still far from the case, even though more awareness has been drawn to this problem, Yang says. \u201cWe are finding many disparities across different ages, gender, ethnicity, and intersectional groups.\u201d\nHe and his colleagues agree on the eventual goal, which is to achieve fairness in health care among all populations. But before we can reach that point, they maintain, we still need a better understanding of the sources of unfairness and how they permeate our current system. Reforming the system as a whole will not be easy, they acknowledge. In fact, the title of the paper they introduced at the Honolulu conference, \u201cChange is Hard,\u201d gives some indications as to the challenges that they and like-minded researchers face.\nThis research is funded by the MIT-IBM Watson AI Lab.\n",
  "published": "2023-08-17",
  "timestamp": "2023-08-22T12:35:42.567085"
}