{
  "unique_id": "9b3fc437-d1ce-5ad5-b8a6-1de70da8010e",
  "title": "A simpler method for learning to control a robot",
  "description": "Researchers develop a machine-learning technique that can efficiently learn to control a robot, leading to better performance with fewer data.",
  "link": "https://news.mit.edu/2023/simpler-method-learning-control-robot-0726",
  "blog_text": "Researchers from MIT and Stanford University have devised a new machine-learning approach that could be used to control a robot, such as a drone or autonomous vehicle, more effectively and efficiently in dynamic environments where conditions can change rapidly.\nThis technique could help an autonomous vehicle learn to compensate for slippery road conditions to avoid going into a skid, allow a robotic free-flyer to tow different objects in space, or enable a drone to closely follow a downhill skier despite being buffeted by strong winds.\nThe researchers\u2019 approach incorporates certain structure from control theory into the process for learning a model in such a way that leads to an effective method of controlling complex dynamics, such as those caused by impacts of wind on the trajectory of a flying vehicle. One way to think about this structure is as a hint that can help guide how to control a system.\n\u201cThe focus of our work is to learn intrinsic structure in the dynamics of the system that can be leveraged to design more effective, stabilizing controllers,\u201d says Navid Azizan, the Esther and Harold E. Edgerton Assistant Professor in the MIT Department of Mechanical Engineering and the Institute for Data, Systems, and Society (IDSS), and a member of the Laboratory for Information and Decision Systems (LIDS). \u201cBy jointly learning the system\u2019s dynamics and these unique control-oriented structures from data, we\u2019re able to naturally create controllers that function much more effectively in the real world.\u201d\nUsing this structure in a learned model, the researchers\u2019 technique immediately extracts an effective controller from the model, as opposed to other machine-learning methods that require a controller to be derived or learned separately with additional steps. With this structure, their approach is also able to learn an effective controller using fewer data than other approaches. This could help their learning-based control system achieve better performance faster in rapidly changing environments.\n\u201cThis work tries to strike a balance between identifying structure in your system and just learning a model from data,\u201d says lead author Spencer M. Richards, a graduate student at Stanford University. \u201cOur approach is inspired by how roboticists use physics to derive simpler models for robots. Physical analysis of these models often yields a useful structure for the purposes of control \u2014 one that you might miss if you just tried to naively fit a model to data. Instead, we try to identify similarly useful structure from data that indicates how to implement your control logic.\u201d\nAdditional authors of the paper are Jean-Jacques Slotine, professor of mechanical engineering and of brain and cognitive sciences at MIT, and Marco Pavone, associate professor of aeronautics and astronautics at Stanford. The research will be presented at the International Conference on Machine Learning (ICML).\nLearning a controller\nDetermining the best way to control a robot to accomplish a given task can be a difficult problem, even when researchers know how to model everything about the system.\nA controller is the logic that enables a drone to follow a desired trajectory, for example. This controller would tell the drone how to adjust its rotor forces to compensate for the effect of winds that can knock it off a stable path to reach its goal.\nThis drone is a dynamical system \u2014 a physical system that evolves over time. In this case, its position and velocity change as it flies through the environment. If such a system is simple enough, engineers can derive a controller by hand.\u00a0\nModeling a system by hand intrinsically captures a certain structure based on the physics of the system. For instance, if a robot were modeled manually using differential equations, these would capture the relationship between velocity, acceleration, and force. Acceleration is the rate of change in velocity over time, which is determined by the mass of and forces applied to the robot.\nBut often the system is too complex to be exactly modeled by hand. Aerodynamic effects, like the way swirling wind pushes a flying vehicle, are notoriously difficult to derive manually, Richards explains. Researchers would instead take measurements of the drone\u2019s position, velocity, and rotor speeds over time, and use machine learning to fit a model of this dynamical system to the data. But these approaches typically don\u2019t learn a control-based structure. This structure is useful in determining how to best set the rotor speeds to direct the motion of the drone over time.\nOnce they have modeled the dynamical system, many existing approaches also use data to learn a separate controller for the system.\n\u201cOther approaches that try to learn dynamics and a controller from data as separate entities are a bit detached philosophically from the way we normally do it for simpler systems. Our approach is more reminiscent of deriving models by hand from physics and linking that to control,\u201d Richards says.\nIdentifying structure\nThe team from MIT and Stanford developed a technique that uses machine learning to learn the dynamics model, but in such a way that the model has some prescribed structure that is useful for controlling the system.\nWith this structure, they can extract a controller directly from the dynamics model, rather than using data to learn an entirely separate model for the controller.\n\u201cWe found that beyond learning the dynamics, it\u2019s also essential to learn the control-oriented structure that supports effective controller design. Our approach of learning state-dependent coefficient factorizations of the dynamics has outperformed the baselines in terms of data efficiency and tracking capability, proving to be successful in efficiently and effectively controlling the system\u2019s trajectory,\u201d Azizan says.\u00a0\nWhen they tested this approach, their controller closely followed desired trajectories, outpacing all the baseline methods. The controller extracted from their learned model nearly matched the performance of a ground-truth controller, which is built using the exact dynamics of the system.\n\u201cBy making simpler assumptions, we got something that actually worked better than other complicated baseline approaches,\u201d Richards adds.\nThe researchers also found that their method was data-efficient, which means it achieved high performance even with few data. For instance, it could effectively model a highly dynamic rotor-driven vehicle using only 100 data points. Methods that used multiple learned components saw their performance drop much faster with smaller datasets.\nThis efficiency could make their technique especially useful in situations where a drone or robot needs to learn quickly in rapidly changing conditions.\nPlus, their approach is general and could be applied to many types of dynamical systems, from robotic arms to free-flying spacecraft operating in low-gravity environments.\nIn the future, the researchers are interested in developing models that are more physically interpretable, and that would be able to identify very specific information about a dynamical system, Richards says. This could lead to better-performing controllers.\n\u201cDespite its ubiquity and importance, nonlinear feedback control remains an art, making it especially suitable for data-driven and learning-based methods. This paper makes a significant contribution to this area by proposing a method that jointly learns system dynamics, a controller, and control-oriented structure,\u201d says Nikolai Matni, an assistant professor in the Department of Electrical and Systems Engineering at the University of Pennsylvania, who was not involved with this work. \u201cWhat I found particularly exciting and compelling was the integration of these components into a joint learning algorithm, such that control-oriented structure acts as an inductive bias in the learning process. The result is a data-efficient learning process that outputs dynamic models that enjoy intrinsic structure that enables effective, stable, and robust control. While the technical contributions of the paper are excellent themselves, it is this conceptual contribution that I view as most exciting and significant.\u201d\nThis research is supported, in part, by the NASA University Leadership Initiative and the Natural Sciences and Engineering Research Council of Canada.\n",
  "published": "2023-07-26",
  "timestamp": "2023-08-22T12:35:42.572345"
}