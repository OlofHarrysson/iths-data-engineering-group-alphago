{
  "unique_id": "10398e11-bcdc-5518-844a-78583082cef3",
  "title": "MIT researchers make language models scalable self-learners",
  "description": "The scientists used a natural language-based logical inference dataset to create smaller language models that outperformed much larger counterparts. ",
  "link": "https://news.mit.edu/2023/language-models-scalable-self-learners-0608",
  "blog_text": "Socrates once said: \u201cIt is not the size of a thing, but the quality that truly matters. For it is in the nature of substance, not its volume, that true value is found.\u201d\nDoes size always matter for large language models (LLMs)? In a technological landscape bedazzled by LLMs taking center stage, a team of MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) researchers think smaller models shouldn\u2019t be overlooked, especially for natural language understanding products widely deployed in the industry.\nTo that end, the researchers cooked up an approach to long-standing problems of inefficiency and privacy associated with big, text-based AI models \u2014 a logic-aware model that outperforms 500-times-bigger counterparts on some language understanding tasks without human-generated annotations, while preserving privacy and robustness with high performance.\nLLMs, which have shown some promising skills in generating language, art, and code, are computationally expensive, and their data requirements can risk privacy leaks when using application programming interfaces for data upload. Smaller models have been historically less capable, particularly in multitasking and weakly supervised tasks, compared to their larger counterparts.\nSo what\u2019s helping these smaller models act so mighty, then? Something called \u201ctextual entailment,\u201d a way to help these models understand a variety of language tasks, where if one sentence (the premise) is true, then the other sentence (the hypothesis) is likely to be true as well. For example, if the premise is, \u201call cats have tails\u201d then the hypothesis \u201ca tabby cat has a tail\u201d would be entailed by the premise. This concept is used to train an \u201centailment model\u201d that proved to be less biased than other language models, from the team\u2019s previous research. They then created \u201cprompts'' that the models can use to figure out if certain information is entailed by a given sentence or phrase according to different tasks. This method improved the model's ability to adapt to different tasks without any additional training, known as zero-shot adaptation.\nIn the realm of \u201cnatural language understanding,\u201d there are various applications that hinge on determining the relationship between two pieces of text. For example, in sentiment classification, a statement like \u201cI think the movie is good\u201d can be inferred or entailed from a movie review that says, \u201cI like the story and the acting is great,\u201d indicating a positive sentiment. Another is news classification, where the topic of a news article can be inferred from its content. For example, a statement like \u201cthe news article is about sports\u201d can be entailed if the main content of the article reports on an NBA game. The key insight was that many existing natural language understanding tasks could be recast as an entailment (i.e., logical inference in natural language) task.\u00a0\n\u201cOur research is about improving the ability of computer programs to understand and process natural language \u2014 the way humans speak and write. Our self-trained, 350-million-parameter entailment models, without human-generated labels, outperform supervised language models with 137 to 175 billion parameters,\u201d says MIT CSAIL postdoc Hongyin Luo, lead author on a new paper about the study. \u201cThis has potential to reshape the landscape of AI and machine learning, providing a more scalable, trustworthy, and cost-effective solution to language modeling,\u201d says Luo. \u201cBy proving that smaller models can perform at the same level as larger ones for language understanding, this work paves the way for more sustainable and privacy-preserving AI technologies.\u201d\u00a0\nThe team discovered that they could improve the model's performance even more by using a technique called \u201cself-training,\u201d where the model uses its own predictions to teach itself, effectively learning without human supervision and additional annotated training data.The self-training method significantly improved performance on a bunch of downstream tasks, including sentiment analysis, question-answering, and news classification. It outperformed both Google's LaMDA and FLAN in zero-shot capabilities, GPT models, and other supervised algorithms.\u00a0\nHowever, one challenge with self-training is that the model can sometimes generate incorrect or noisy labels that harm performance. To overcome this, they developed a new algorithm called 'SimPLE' (Simple Pseudo-Label Editing), a process to review and modify the pseudo-labels made in initial rounds of learning. By correcting any mislabeled instances, it improved the overall quality of the self-generated labels. This not only made the models more effective at understanding language, but more robust when faced with adversarial data.\u00a0\nAs with most research, there are some limitations. The self-training on multi-class classification tasks didn't perform as well as on binary natural language understanding tasks, indicating the challenge of applying entailment models to multi-choice tasks.\n\n\u201cThis research presents an efficient and effective way to train large language models (LLMs) by formulating natural language understanding tasks as contextual entailment problems and employing a pseudo-labeling self-training mechanism to incorporate large quantities of unlabelled text data in the training process,\u201d adds CSAIL Senior Research Scientist James Glass, who is also an author on the paper. \u201cWhile the field of LLMs is undergoing rapid and dramatic changes, this research shows that it is possible to produce relatively compact language models that perform very well on benchmark understanding tasks compared to their peers of roughly the same size, or even much larger language models.\u201d\n\u201cEntailment task is a popular proxy to evaluate \u201cunderstanding\u201d of a given context by an AI model,\u201d says Leonid Karlinsky, research staff member at the MIT-IBM Watson AI Lab. \u201cIt is used in many areas analyzing models with unimodal, like LLMs, and and multi-modal, like VLMs [visual language models] inputs, simplifying the task of question-answering about a given input context to a binary classification problem \u2014 does this context entail a certain (e.g., text) conclusion or not? This paper makes two contributions in this space. First, it proposes a way to improve the zero-shot (without additional tuning) NLU performance and robustness to adversarial attacks via tuning with synthesized (specialized) entailment tasks generated for the primal NLU task. Second, it offers a self-supervised SimPLE method including pseudo-labeling and confidence-based filtering to further improve large LLMs' NLU performance.\u201d\nLuo and Glass wrote the paper with Yoon Kim, a CSAIL member and assistant professor in MIT\u2019s Department of Electrical Engineering and Computer Science, and Jiaxin Ge of Peking University. Their work will be presented at the meeting of the Association for Computational Linguistics in Toronto, Ontario this July. This research was supported by a grant from the Hong Kong Innovation AI program.\n",
  "published": "2023-06-08",
  "timestamp": "2023-08-22T12:35:42.610819"
}